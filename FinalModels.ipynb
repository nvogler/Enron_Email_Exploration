{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from pymongo import MongoClient\n",
    "\n",
    "def _connect_mongo(host, port, username, password, db):\n",
    "    \"\"\" A util for making a connection to mongo \"\"\"\n",
    "\n",
    "    if username and password:\n",
    "        mongo_uri = 'mongodb://%s:%s@%s:%s/%s' % (username, password, host, port, db)\n",
    "        conn = MongoClient(mongo_uri)\n",
    "    else:\n",
    "        conn = MongoClient(host, port)\n",
    "\n",
    "\n",
    "    return conn[db]\n",
    "\n",
    "def read_mongo(db, collection, query={}, host='localhost', port=27017, username=None, password=None, no_id=True):\n",
    "    \"\"\" Read from Mongo and Store into DataFrame \"\"\"\n",
    "\n",
    "    # Connect to MongoDB\n",
    "    db = _connect_mongo(host=host, port=port, username=username, password=password, db=db)\n",
    "\n",
    "    # Make a query to the specific DB and Collection\n",
    "    cursor = db[collection].find(query)\n",
    "\n",
    "    # Expand the cursor and construct the DataFrame\n",
    "    df =  pd.DataFrame(list(cursor))\n",
    "\n",
    "    # Delete the _id\n",
    "    if no_id:\n",
    "        del df['_id']\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw collections into dataframes\n",
    "email_df = read_mongo('enron', 'emails')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please answer all of the questions below...\n",
      "Are we going to look for a ring tomorrow?   I guess\n",
      "Do you know what size she wears? (I'm working on finding out myself)  i don't know\n",
      "What time frame do you want to work with for an engagement? (Yes a TIME FRAME)  (ie, in a week, month, year...)\n",
      "I have the impression that beth wants you to take care of the whole wedding thing, is this true?\n",
      "She says she wants to go away (deffinately tropical because she has never seen blue water) and invite close family members (parents sibblings) and close friends by sending a letter/invitation.  She says they will need time to save if they are going to come, so I suggested 6 months.\n",
      "Does this sound appealing to you?\n",
      "FYI... My brother went away to get married and I have a friend who went away so I have experience in planning tropical weddings.  They are great because only those who you want there come and others aren't affended for not being invited because y'all went away.  You can have a dream wedding for just a little more than what a nice vacation would cost.  It is alot cheaper than having one here which is probably important if y'all are paying for it.\n",
      "What is my budget for the ring?\n",
      "What is my budget for the wedding?\n",
      "Please answer all of the questions above to the best of your ability.  This is crucial to the success of your engagement.  Thanks for your help!\n",
      "Your Friendly Wedding Consultant\n",
      "Ashley Landry\n",
      "EnFORM Technology, LLC\n",
      "Office: (713) 350-1933\n",
      "Fax: (713) 438-1933\n",
      "Email: ashley.landry@enform.com\n"
     ]
    }
   ],
   "source": [
    "print (email_df['text_chunks'][1][0]['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E-Mail Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# E-mails: 276279\n",
      "# E-mails w/From: 263587\n"
     ]
    }
   ],
   "source": [
    "# Format e-mail BSON to usable data frame\n",
    "email_df = email_df[['from',\n",
    "                     'to',\n",
    "                     'header_info',\n",
    "                     'subject',\n",
    "                     'text_chunks']]\n",
    "print (\"# E-mails: \" + str(email_df.shape[0]))\n",
    "email_df = email_df[email_df['from'] > 0]\n",
    "print (\"# E-mails w/From: \" + str(email_df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entity Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Entities: 95941\n",
      "# Entities with position nodes: 3187\n"
     ]
    }
   ],
   "source": [
    "# Format entity BSON to usable data frame\n",
    "ent_df = read_mongo('enron', 'entities')\n",
    "ent_df = ent_df[['email_address',\n",
    "                'email_addresses',\n",
    "                'position',\n",
    "                'position_id',\n",
    "                'position_nodes',\n",
    "                ]]\n",
    "print (\"# Entities: \" + str(ent_df.shape[0]))\n",
    "\n",
    "# Remove null affiliated employees\n",
    "#ent_df_no_pn = ent_df[ent_df['position_nodes'].isnull()]\n",
    "#ent_df = ent_df[ent_df['affiliation'].notnull()]\n",
    "#print (\"# Non-null-affiliated entities: \" + str(ent_df.shape[0]))\n",
    "ent_df = ent_df[ent_df['position_nodes'].notnull()]\n",
    "\n",
    "print (\"# Entities with position nodes: \" + str(ent_df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New Ranking System Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_titles = set([])\n",
    "jt_ids = set([])\n",
    "for i, row in ent_df.iterrows():\n",
    "    try:\n",
    "        job_titles.add(row['position'])\n",
    "        jt_ids.add(row['position_id'])\n",
    "    except:\n",
    "        None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_titles_both = defaultdict(set)\n",
    "for i, row in ent_df.iterrows():\n",
    "    #try:\n",
    "    job_titles_both[row['position']].add(row['position_id'])\n",
    "    #except:\n",
    "    #    None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "368\n",
      "125\n"
     ]
    }
   ],
   "source": [
    "jt_map = {}\n",
    "rank_map = {}\n",
    "i = 0\n",
    "\n",
    "for jt in job_titles:\n",
    "    jt = str(jt)\n",
    "    if jt.find('CEO') != -1:\n",
    "        jt_map[jt] = 'CEO'\n",
    "        rank_map[jt] = 0\n",
    "    elif jt.find('COO') != -1 or jt.find('CTO') != -1 or \\\n",
    "            jt.find('CFO') != -1 or \\\n",
    "            (jt.find('Chief') != -1 and jt.find('Officer') != -1):\n",
    "        jt_map[jt] = 'C-Suite'\n",
    "        rank_map[jt] = 0\n",
    "    elif jt.find('Vice') != -1 or jt.find('VP') != -1:\n",
    "        jt_map[jt] = 'VP'\n",
    "        rank_map[jt] = 3\n",
    "    elif jt.find('President') != -1:\n",
    "        jt_map[jt] = 'President'\n",
    "        rank_map[jt] = 2\n",
    "    elif jt.find('Lawyer') != -1 or jt.find('Counsel') != -1 or jt.find('Legal') != -1 or jt.find('Attorney') != -1:\n",
    "        jt_map[jt] = 'Lawyer'\n",
    "        rank_map[jt] = 4\n",
    "    elif jt.find('Director') != -1:\n",
    "        rank_map[jt] = 4\n",
    "        jt_map[jt] = 'Director'\n",
    "    elif jt.find('Manager') != -1:\n",
    "        jt_map[jt] = 'Manager'\n",
    "        rank_map[jt] = 5\n",
    "    elif jt.find('Analyst') != -1 or jt.find('Specialist') != -1 or jt.find('Engineer') != -1 or \\\n",
    "            jt.find('Tech') != -1:\n",
    "        jt_map[jt] = 'Tech'\n",
    "        rank_map[jt] = 6\n",
    "    elif jt.find('Assistant') != -1 or jt.find('HR') != -1 or jt.find('Secretary') != -1 or \\\n",
    "            jt.find('Admin') != -1 or jt.find('Assoc') != -1 or jt.find('Clerk') != -1 or jt.find('Entry') != -1:\n",
    "        jt_map[jt] = 'Admin'\n",
    "        rank_map[jt] = 6\n",
    "    elif jt.find('Trader') != -1:\n",
    "            jt_map[jt] = 'Trader'\n",
    "            rank_map[jt] = 6\n",
    "    else:\n",
    "        jt_map[jt] = 'Unknown'\n",
    "        rank_map[jt] = -1\n",
    "        i += 1\n",
    "\n",
    "print (len(job_titles))\n",
    "print (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_to_jt = defaultdict(int)\n",
    "k = 0\n",
    "l = 0\n",
    "for i, row in ent_df.iterrows():\n",
    "    if str(row['email_address']) != 'nan' and str(row['position']) != 'nan':\n",
    "        email_to_jt[row['email_address'].lower()] = rank_map[row['position']]\n",
    "        for em_add in row['email_addresses']:\n",
    "            email_to_jt[em_add.lower()] = rank_map[row['position']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Email Addresses Mapped: 1518\n",
      "Remaining Unknown: 0\n"
     ]
    }
   ],
   "source": [
    "# Double check for still unknowns\n",
    "i = 0\n",
    "j = 0\n",
    "for h, row in ent_df.iterrows():\n",
    "    if str(row['email_address']) != 'nan':\n",
    "        if email_to_jt[row['email_address'].lower()] == -1:\n",
    "            jt = row['position_id']\n",
    "            \n",
    "            if jt.find('CEO') != -1:\n",
    "                email_to_jt[row['email_address'].lower()] = 0\n",
    "            elif jt.find('COO') != -1 or jt.find('CTO') != -1 or \\\n",
    "                    jt.find('CFO') != -1 or \\\n",
    "                    (jt.find('Chief') != -1 and jt.find('Officer') != -1):\n",
    "                email_to_jt[row['email_address'].lower()] = 1\n",
    "            elif jt.find('Vice') != -1 or jt.find('VP') != -1:\n",
    "                email_to_jt[row['email_address'].lower()] = 3\n",
    "            elif jt.find('President') != -1:\n",
    "                email_to_jt[row['email_address'].lower()] = 2\n",
    "            elif jt.find('Lawyer') != -1 or jt.find('Counsel') != -1 or jt.find('Legal') != -1 or jt.find('Attorney') != -1:\n",
    "                email_to_jt[row['email_address'].lower()] = 4\n",
    "            elif jt.find('Director') != -1:\n",
    "                email_to_jt[row['email_address'].lower()] = 4\n",
    "            elif jt.find('Manager') != -1:\n",
    "                email_to_jt[row['email_address'].lower()] = 5\n",
    "            elif jt.find('Analyst') != -1 or jt.find('Specialist') != -1 or jt.find('Engineer') != -1 or \\\n",
    "                    jt.find('Tech') != -1 or jt.find('Logistician') != -1 or jt.find('Operator') != -1 or \\\n",
    "                    jt.find('Statistician') != -1 or jt.find('Designer') != -1 or jt.find('Producer') != -1 or \\\n",
    "                    jt.find('Purchasing Agent') != -1:\n",
    "                email_to_jt[row['email_address'].lower()] = 6\n",
    "            elif jt.find('Assistant') != -1 or jt.find('HR') != -1 or jt.find('Secretary') != -1 or \\\n",
    "                    jt.find('Admin') != -1 or jt.find('Assoc') != -1 or jt.find('Clerk') != -1 or \\\n",
    "                    jt.find('Customer Service') != -1 or jt.find('Recruit') != -1 or jt.find('Entry') != -1:\n",
    "                email_to_jt[row['email_address'].lower()] = 6\n",
    "            elif jt.find('Trader') != -1:\n",
    "                email_to_jt[row['email_address'].lower()] = 6\n",
    "            else:\n",
    "                i += 1\n",
    "                j -= 1\n",
    "        j += 1\n",
    "print (\"Email Addresses Mapped: \" + str(j))\n",
    "print (\"Remaining Unknown: \" + str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6154\n"
     ]
    }
   ],
   "source": [
    "#for i, email in email_df.iterrows():\n",
    "#    if 'content' in email['text_chunks'][0].keys():\n",
    "#        print (email['text_chunks'][0]['content'])\n",
    "#    if i > 1010: break\n",
    "print (len(email_to_jt.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 unranked messages.\n"
     ]
    }
   ],
   "source": [
    "# Employee Email to Position Code\n",
    "u_count = 0\n",
    "n_count = 0\n",
    "s_count = 0\n",
    "d_count = 0\n",
    "\n",
    "conversation_strings = defaultdict(str)\n",
    "conversation_count = defaultdict(int)\n",
    "conversation_labels = defaultdict(str)\n",
    "\n",
    "for i, email in email_df.iterrows():\n",
    "    \n",
    "    # Check if valid role\n",
    "    from_ = None\n",
    "    to_ = []\n",
    "    for header in email['header_info']:\n",
    "        if header['role'] == 'from':\n",
    "            from_ = header['email_address'].lower()\n",
    "        elif header['role'] == 'to':\n",
    "            to_.append(header['email_address'].lower())\n",
    "    if from_ is None:\n",
    "        continue\n",
    "\n",
    "    content = \"\"\n",
    "    if 'content' in email['text_chunks'][0].keys():\n",
    "        content += email['text_chunks'][0]['content']\n",
    "    if str(email['subject']) != 'nan':\n",
    "        content += email['subject'] + \" \"\n",
    "    \n",
    "    #if len(to_) > 8:\n",
    "    #    continue\n",
    "        \n",
    "    for to_address in to_:\n",
    "        if from_ != to_address:\n",
    "            if to_address in email_to_jt .keys() and from_ in email_to_jt.keys():\n",
    "                code1 = email_to_jt[from_]\n",
    "                code2 = email_to_jt[to_address]\n",
    "                \n",
    "                if code1 == -1 or code2 == -1:\n",
    "                    continue\n",
    "                    \n",
    "                index_ = (from_, to_address)\n",
    "                \n",
    "                u_count = 0\n",
    "                n_count = 0\n",
    "                s_count = 0\n",
    "                d_count = 0\n",
    "                \n",
    "                conversation_strings[index_] += \" \" + content.lower()\n",
    "                conversation_count[index_] += 1\n",
    "                    \n",
    "                if code1 < code2:\n",
    "                    conversation_labels[index_] = 'up'\n",
    "                    u_count += 1\n",
    "                elif code1 > code2:\n",
    "                    conversation_labels[index_] = 'down'\n",
    "                    d_count += 1\n",
    "                elif code1 == code2:\n",
    "                    conversation_labels[index_] = 'neutral'\n",
    "                    s_count += 1  \n",
    "                else:\n",
    "                    conversation_labels[index_] = 'unk'\n",
    "                    n_count += 1\n",
    "print (str(n_count) + \" unranked messages.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total relationships: 36027\n",
      "Avg char. count of content: 4525.337052765981\n",
      "\n",
      "Total e-mails sent: 192538\n",
      "Avg convo. count per relationship: 5.344269575596081\n"
     ]
    }
   ],
   "source": [
    "total_length = len(conversation_strings.keys())\n",
    "total_count = 0\n",
    "convos_count = 0\n",
    "\n",
    "for c_ in conversation_count:\n",
    "    convos_count += conversation_count[c_]\n",
    "\n",
    "for rc in conversation_strings:\n",
    "    total_count += len(conversation_strings[rc])\n",
    "    \n",
    "print (\"Total relationships: \" + str(total_length))\n",
    "print (\"Avg char. count of content: \" + str(total_count / total_length))\n",
    "\n",
    "print (\"\\nTotal e-mails sent: \" + str(convos_count))\n",
    "print (\"Avg convo. count per relationship: \" + str(convos_count / total_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to label -> text\n",
    "u_count = 0\n",
    "nu_count = 0\n",
    "n_count = 0\n",
    "for key in conversation_strings.keys():\n",
    "    \n",
    "    if conversation_labels[key] == 'up':\n",
    "        u_count += 1\n",
    "    elif conversation_labels[key] == 'down':\n",
    "        nu_count += 1\n",
    "    elif conversation_labels[key] == 'neutral':\n",
    "        n_count += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upward messages: 9970\n",
      "Not-upward messages: 11507\n",
      "Neutral messages: 14550\n"
     ]
    }
   ],
   "source": [
    "print (\"Upward messages: \" + str(u_count))\n",
    "print (\"Not-upward messages: \" + str(nu_count))\n",
    "print (\"Neutral messages: \" + str(n_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleansing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Nic\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Nic\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import string\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "import time \n",
    "\n",
    "# Some code from: https://medium.com/@sabber/classifying-yelp-review-comments\n",
    "# used to create this function\n",
    "def clean_text(text):\n",
    "    \n",
    "    ## Remove puncuation\n",
    "    #text = text.translate(string.punctuation)\n",
    "    \n",
    "    ## Convert words to lower case and split them\n",
    "    text = text.lower().split()\n",
    "    \n",
    "    ## Remove stop words\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    text = [w for w in text if not w in stops and len(w) >= 2]\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "    \n",
    "    ## Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(\"\\d+\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" \", text)\n",
    "    text = re.sub(r\"\\+\", \" \", text)\n",
    "    text = re.sub(r\"\\@\", \" \", text)\n",
    "    text = re.sub(r\"\\-\", \" \", text)\n",
    "    text = re.sub(r\"\\=\", \" \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", \" \", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \" \", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    \n",
    "    text = text.replace(\"  \", \" \")\n",
    "    text = text.split()\n",
    "    text = [w for w in text if len(w) >= 3 and len(set(w)) > 1]\n",
    "    text = \" \".join(text)\n",
    "    \n",
    "    ## Stemming\n",
    "    #text = text.split()\n",
    "    #stemmer = SnowballStemmer('english')\n",
    "    #stemmed_words = [stemmer.stem(word) for word in text]\n",
    "    #text = \" \".join(stemmed_words)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 36027 relationships...\n",
      "Text processing completed in 30.2667236328125 seconds.\n"
     ]
    }
   ],
   "source": [
    "post_prep_format = []\n",
    "i = 0\n",
    "start = time.time()\n",
    "print (\"Processing \" + str(len(conversation_strings.keys())) + \" relationships...\")\n",
    "for key in conversation_strings.keys():\n",
    "    i += 1\n",
    "    label = conversation_labels[key]\n",
    "    \n",
    "    if label == 'up' or label == 'down':\n",
    "        \n",
    "        content = clean_text(conversation_strings[key])\n",
    "        \n",
    "        post_prep_format.append((label, content))\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print (\"At position...\" + str(i), end='\\r')\n",
    "        \n",
    "finish = time.time()\n",
    "print(\"Text processing completed in \" + str(finish-start) + \" seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10906\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(post_prep_format, columns=['label', 'message'])\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "df_train = df[:round(df.shape[0] *.7)]\n",
    "df_test = df[round(df.shape[0] *.7):]\n",
    "\n",
    "df_test = df_test.reset_index()\n",
    "\n",
    "print ((df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "for message in df['message']:\n",
    "    if message.find('0') != -1:\n",
    "        print (message)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(2,6))\n",
    "clf = MultinomialNB()\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "baseline_clf = Pipeline([('vect', vectorizer),\n",
    "                         ('tfidf', TfidfTransformer()),\n",
    "                         ('clf', clf),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(2, 6), preprocessor=None, stop_words=None,\n",
       "        strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_clf.fit(df_train['message'], df_train['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial baseline accuracy: 0.66320293398533\n"
     ]
    }
   ],
   "source": [
    "predicted = baseline_clf.predict(df_test['message'])\n",
    "print (\"Initial baseline accuracy: \" + str(np.mean(predicted == df_test['label'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_most_informative_features(vectorizer, clf, n=20):\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    coefs_with_fns = sorted(zip(clf.coef_[0], feature_names))\n",
    "    top = zip(coefs_with_fns[:n], coefs_with_fns[:-(n + 1):-1])\n",
    "    for (coef_1, fn_1), (coef_2, fn_2) in top:\n",
    "        print (\"\\t%.4f\\t%-15s\\t\\t%.4f\\t%-15s\" % (coef_1, fn_1, coef_2, fn_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t-15.5771\taae dellnet    \t\t-12.8929\tlet know       \n",
      "\t-15.5771\taae dellnet combliley\t\t-12.9599\twould like     \n",
      "\t-15.5771\taae dellnet combliley transmission\t\t-13.2567\tplease let     \n",
      "\t-15.5771\taae dellnet combliley transmission language\t\t-13.3358\tplease let know\n",
      "\t-15.5771\taae dellnet combliley transmission language case\t\t-13.4559\tstaff meeting  \n",
      "\t-15.5771\taae dellnet combliley transmission language commerce\t\t-13.4875\tsee attached   \n",
      "\t-15.5771\taae dellnet combliley transmission language richardson\t\t-13.5189\tplease see     \n",
      "\t-15.5771\taae dellnet comlanguage\t\t-13.5451\tmaster agreement\n",
      "\t-15.5771\taae dellnet comlanguage bliley\t\t-13.5470\tnorth america  \n",
      "\t-15.5771\taae dellnet comlanguage bliley sent\t\t-13.6132\tenron north america\n"
     ]
    }
   ],
   "source": [
    "show_most_informative_features(vectorizer, clf, n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What's going wrong?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "down\n",
      "dear john attached trading contract package natsourcedirect trading platform based proven global vision software trayport believe strongly able serve better combining skills brokers line capabilities natsourcedirect able trade system need sign standard participant agreement read market rules procedures complete schedule please mail fax signed agreement new york office attention able provide trading access natsourcedirect questions please hesitate call email tclaughton natsource com mail natsource llc natsourcedirect division attn trent claughton broadway floor fax natsourcedirect division attn trent claughton best regards trent claughton need adobe acrobat read print documents adobe acrobat reader downloaded free http adobe com copies fedexed email tclaughton natsource comnatsourcedirect trading contract\n",
      "up\n",
      "ken responsible managing enron banking relationships speaking rosalee reserving time calendar meetings top banks given uncertainty tightness credit markets andy ben discussed importance getting andy front senior members management jpmorgan chase citigroup csfb deutsche bank expecting institutions play important role achieving year end financing plan rosalee indicated may time available tuesday november meet new york please let know make calendar available meetings thank you kellybank meetings new york\n",
      "down\n",
      "presentation\n",
      "up\n",
      "conference call agenda daily leadership agenda conference call agenda enron risk management seminar take place thursday april new york city are enron invitation enron letter policymakers list policymakers planning attend list wholesale customers invited schedule presenation list policy makers attending assemblyman paul tonko john howard chief staff assemblyman tonko john cahill senior policy advisor govenor pataki mary ellen burns assistant attorney general energy division gavin donohue independent power producers new york richard miller senior energy city economic development corporation just aside power traders met separately large group psc staffers march discuss risk management updated invite list schedule presenation michael brown power outlook minutes craig breslau natural gas outlook minutes janelle scheuer hedging strategies hour gary taylor weather risk management minutes binders presentations made available attendeesny risk management seminar leadership call agenda http edisoninvestor com financialexc sce mou pdfmou socal edison gov davis\n",
      "up\n",
      "justin edmund attached agreement back back transactions agency services provided please call either heather queries heather direct line direct line kind regards ursula hap enron agreement doc message confidential may also privileged otherwise protected work product immunity legal rules received mistake please let know reply delete system; copy message disclose contents anyone enron transaction guys please find attached revised gtcs per discussions last days could please review agree amendments and comments please forward comments back close business tomorrow friday june finalised versions ready sign monday regards amitametal gtc dear alan mark attached spreadsheet containing two lists one containing existing enrononline counterparties; one containing existing metalgesellschaft counterparties justin lists marked unacceptable with counterparties jurisdictions currently unable trade physical commodities via enrononline respect canadian counterparties assume difficulties trading physically settled metals commodities would appreciate thoughts clearance issue let know need information this best regards metals cps attached information regards edmumdchina due diligence dear jeff mark justin discussing way approach variation eta respect counterparties existing master agreement want transactions governed applicable gtc product question initial plan issue letter counterparty asking sign return letter agreement would vary provision eta states master agreement govern transactions reformulated position drafted letter attached below simply sent relevant counterparty require sign return documents believe letter provide effective variation eta stating counterparty acceptance gtc eol system mean gtc govern transactions master agreement think need make explicit reference eta obviously work counterparty presented accept gtc button usual juncture system addition thought best remove explantory paragraph stating writing counterparty future date revise credit provisions master the whole reason counterparty accept gtc master obviously welcome thoughts issue attached draft also presume need take view whether varying eta way effective new york law best regards online trading gtc master variation final dare say credit gtcs reflecting recent final comments without collateral collateraleol final credit product gtcs dear mark hope well have attached confirmation eol bankruptcy swaps ena contracting entity have followed format online ena swap confirm kept economics european bankruptcy swap confirm please let know happy this thanks edmund eol credit product ena confirmation trying follow reliant concerning desire trade office netherlands status eta netherlands netherlands eta edmund reviewed long descriptions given credit derivatives not tell current substantial comments attached come sorry pieces basically think restating something long description already covered sometimes different language gtc creates ambiguity necessary form complete contract concern able draft language fix not find definition seller payment amount think buyer payment language works give call discuss concerns eol credit derivatives long descriptions regarding phone request conflicts check respect duke energy affiliates find following executed agreements duke energy field services inc houston pipe line company regarding hpl three rivers plant effective duke energy field services inc ect regarding proposed expension dauphin island gathering systemor duke related pipelines serve ect gas markets effective duke energy north america ect transaction involving duke supply long term power capacity ect support ect wholesale supply obligation within nepool effective duke energy north america ena possible duke acquisition ena interests natural gas fired generation facility developed port morow oregon effective duke energy power services ect regarding natural gas fired electric power generation facilities development ect illinois indiana kentucky mississippi tennessee effective duke power company enron power marketing inc regarding provision central control area scheduling services ect affiliates duke state georgia confidentiality query\n",
      "up\n",
      "john quick comment seating arrangements weather group understand new plan put gas power weather guys one location recommendation objective two separate weather groups put place system checks balances encourage independent views new seating arrangements nullify power weather guys defer mike unquestionable authority experience ahead new seating arrangements makes sense combine two groups mike leadership please let know think vinceweather group seats trading floors john sure already sent resume please take look met claude years ago greatly impressed him may good candidate work andy zipper vince <<claude philoche doc resume john sure already sent resume please take look met claude years ago greatly impressed him may good candidate work andy zipper vince <<claude philoche doc resume john sure already sent resume please take look met claude years ago greatly impressed him may good candidate work andy zipper resume john mills phone number vinceed mills phone number recommendations\n",
      "down\n",
      "tom heard today arriving todd hall apartment please let know going first heard think bit late considering thought today mrpmove http goldmansachs com ficc confirmations did see this\n",
      "Predicted up: 5\n",
      "Actual up: 4\n",
      "Predicted down: 8\n",
      "Actual down: 9\n"
     ]
    }
   ],
   "source": [
    "# Print 5 messages of misclassified samples\n",
    "import math\n",
    "test_start_index = math.floor(df.shape[ 0] * .7)\n",
    "k = 0\n",
    "j = 0\n",
    "l = 0\n",
    "q = 0\n",
    "m = 0\n",
    "for i, x in enumerate(predicted):\n",
    "    if x == 'up':\n",
    "        j += 1\n",
    "    else:\n",
    "        k += 1\n",
    "    \n",
    "    if df_test['label'][i] == 'up':\n",
    "        l += 1\n",
    "    else:\n",
    "        m += 1\n",
    "    if x != df_test['label'][i]:\n",
    "        print (x)\n",
    "        print (df_test['message'][i])\n",
    "        if q > 5:\n",
    "            break\n",
    "        else:\n",
    "            q += 1\n",
    "print (\"Predicted up: \" + str(j))\n",
    "print (\"Actual up: \" + str(l))\n",
    "print (\"Predicted down: \" + str(k))\n",
    "print (\"Actual down: \" + str(m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 53546\n"
     ]
    }
   ],
   "source": [
    "words = set()\n",
    "for message in df_train['message']:\n",
    "    for word in message.split():\n",
    "        words.add(word)\n",
    "vocab_size = len(words)\n",
    "print (\"Vocab size: \" + str(vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(post_prep_format, columns=['label', 'message'])\n",
    "df = df.drop_duplicates()\n",
    "j = 0\n",
    "k = 0\n",
    "binary = False\n",
    "if binary:\n",
    "    for i, row in df.iterrows():\n",
    "        if row['label'] == 'up':\n",
    "            row['label'] = 1\n",
    "            j += 1\n",
    "        else:\n",
    "            row['label'] = 0\n",
    "            k += 1\n",
    "\n",
    "df_train = df[:round(df.shape[0] *.7)]\n",
    "df_test = df[round(df.shape[0] *.7):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import sklearn.datasets as skds\n",
    "from pathlib import Path\n",
    "# Keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras import optimizers\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "num_labels = 2\n",
    "batch_size = 100\n",
    " \n",
    "# define Tokenizer with Vocab Size\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(df_train['message'])\n",
    " \n",
    "x_train = tokenizer.texts_to_matrix(df_train['message'], mode='tfidf')\n",
    "x_test = tokenizer.texts_to_matrix(df_test['message'], mode='tfidf')\n",
    " \n",
    "encoder = LabelBinarizer()\n",
    "encoder.fit(df_train['label'])\n",
    "y_train = encoder.transform(df_train['label'])\n",
    "y_test = encoder.transform(df_test['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_22 (Dense)             (None, 1024)              54832128  \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 2)                 2050      \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 55,883,778\n",
      "Trainable params: 55,883,778\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(1024, input_shape=(vocab_size,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(1024))\n",
    "model.add(Activation('softmax'))\n",
    "model.add(Dense(num_labels))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=3,\n",
    "                    verbose=0,\n",
    "                    validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.6941606323701537\n",
      "Test accuracy: 0.5241442542787286\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(post_prep_format, columns=['label', 'message'])\n",
    "df = df.drop_duplicates()\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "j = 0\n",
    "k = 0\n",
    "binary = True\n",
    "if binary:\n",
    "    for i, row in df.iterrows():\n",
    "        if row['label'] == 'up':\n",
    "            row['label'] = 1\n",
    "            j += 1\n",
    "        else:\n",
    "            row['label'] = 0\n",
    "            k += 1\n",
    "\n",
    "df_train = df[:round(df.shape[0] *.7)]\n",
    "df_test = df[round(df.shape[0] *.7):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create sequence\n",
    "tokenizer = Tokenizer(num_words= vocab_size)\n",
    "tokenizer.fit_on_texts(df_train['message'])\n",
    "sequences = tokenizer.texts_to_sequences(df_train['message'])\n",
    "data = pad_sequences(sequences, maxlen=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4580 samples, validate on 3054 samples\n",
      "Epoch 1/3\n",
      "4580/4580 [==============================] - 19s 4ms/step - loss: 0.6753 - acc: 0.5683 - val_loss: 0.6599 - val_acc: 0.5868\n",
      "Epoch 2/3\n",
      "4580/4580 [==============================] - 17s 4ms/step - loss: 0.5111 - acc: 0.7611 - val_loss: 0.6684 - val_acc: 0.5861\n",
      "Epoch 3/3\n",
      "4580/4580 [==============================] - 18s 4ms/step - loss: 0.3471 - acc: 0.8563 - val_loss: 0.7884 - val_acc: 0.5982\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2370b2585f8>"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Network architecture\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 100, input_length=50))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "## Fit the model\n",
    "model.fit(data, np.array(df_train['label']), validation_split=0.4, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.8987463428513637\n",
      "Test accuracy: 0.48746943765281175\n"
     ]
    }
   ],
   "source": [
    "tokenizer.fit_on_texts(df_test['message'])\n",
    "sequences_t = tokenizer.texts_to_sequences(df_test['message'])\n",
    "data_t = pad_sequences(sequences_t, maxlen=50)\n",
    "score = model.evaluate(data_t, np.array(df_test['label']), verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM-CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create sequence\n",
    "tokenizer = Tokenizer(num_words= vocab_size)\n",
    "tokenizer.fit_on_texts(df_train['message'])\n",
    "sequences = tokenizer.texts_to_sequences(df_train['message'])\n",
    "data = pad_sequences(sequences, maxlen=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4580 samples, validate on 3054 samples\n",
      "Epoch 1/3\n",
      "4580/4580 [==============================] - 136s 30ms/step - loss: 0.6817 - acc: 0.5393 - val_loss: 0.6579 - val_acc: 0.5714\n",
      "Epoch 2/3\n",
      "4580/4580 [==============================] - 124s 27ms/step - loss: 0.5264 - acc: 0.7378 - val_loss: 0.7195 - val_acc: 0.5910\n",
      "Epoch 3/3\n",
      "4580/4580 [==============================] - 124s 27ms/step - loss: 0.3301 - acc: 0.8539 - val_loss: 0.8247 - val_acc: 0.5930\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2370fd70358>"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 100, input_length=50))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=8))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam',    metrics=['accuracy'])\n",
    "model.fit(data, np.array(df_train['label']), validation_split=0.4, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.9201591125618858\n",
      "Test accuracy: 0.508557457212714\n"
     ]
    }
   ],
   "source": [
    "tokenizer.fit_on_texts(df_test['message'])\n",
    "sequences_t = tokenizer.texts_to_sequences(df_test['message'])\n",
    "data_t = pad_sequences(sequences_t, maxlen=50)\n",
    "score = model.evaluate(data_t, np.array(df_test['label']), verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create sequence\n",
    "tokenizer = Tokenizer(num_words= vocab_size)\n",
    "tokenizer.fit_on_texts(df_train['message'])\n",
    "sequences = tokenizer.texts_to_sequences(df_train['message'])\n",
    "data = pad_sequences(sequences, maxlen=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 100, input_length=50))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv1D(64, 5, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "sgd =optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "model.fit(data, np.array(df_train['label']), validation_split=0.3, epochs=3)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### SciKit Learn Default Models\n",
    "##### Adapted for this data set with some additional slight modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nic\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "from optparse import OptionParser\n",
    "import sys\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils.extmath import density\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(post_prep_format, columns=['label', 'message'])\n",
    "full = df.shape[0]\n",
    "df = df.drop_duplicates()\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df_train = df[:round(df.shape[0] *.7)]\n",
    "df_test = df[round(df.shape[0] *.7):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df_train['label']\n",
    "y_test = df_test['label']\n",
    "\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,\n",
    "                             stop_words='english')\n",
    "X_train = vectorizer.fit_transform(df_train['message'])\n",
    "X_test = vectorizer.transform(df_test['message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(clf):\n",
    "    print('_' * 80)\n",
    "    print(\"Training: \")\n",
    "    print(clf)\n",
    "    t0 = time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_time = time() - t0\n",
    "    print(\"train time: %0.3fs\" % train_time)\n",
    "\n",
    "    t0 = time()\n",
    "    pred = clf.predict(X_test)\n",
    "    test_time = time() - t0\n",
    "    print(\"test time:  %0.3fs\" % test_time)\n",
    "\n",
    "    score = metrics.accuracy_score(y_test, pred)\n",
    "    print(\"accuracy:   %0.3f\" % score)\n",
    "\n",
    "    print()\n",
    "    clf_descr = str(clf).split('(')[0]\n",
    "    return clf_descr, score, train_time, test_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Ridge Classifier\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,\n",
      "        max_iter=None, normalize=False, random_state=None, solver='lsqr',\n",
      "        tol=0.01)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nic\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\ridge.py:319: UserWarning: In Ridge, only 'sag' solver can currently fit the intercept when X is sparse. Solver has been automatically changed into 'sag'.\n",
      "  warnings.warn(\"In Ridge, only 'sag' solver can currently fit the \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train time: 0.585s\n",
      "test time:  0.003s\n",
      "accuracy:   0.656\n",
      "\n",
      "================================================================================\n",
      "Perceptron\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "Perceptron(alpha=0.0001, class_weight=None, eta0=1.0, fit_intercept=True,\n",
      "      max_iter=None, n_iter=50, n_jobs=1, penalty=None, random_state=0,\n",
      "      shuffle=True, tol=None, verbose=0, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nic\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train time: 0.289s\n",
      "test time:  0.003s\n",
      "accuracy:   0.619\n",
      "\n",
      "================================================================================\n",
      "Passive-Aggressive\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "PassiveAggressiveClassifier(C=1.0, average=False, class_weight=None,\n",
      "              fit_intercept=True, loss='hinge', max_iter=None, n_iter=50,\n",
      "              n_jobs=1, random_state=None, shuffle=True, tol=None,\n",
      "              verbose=0, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nic\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train time: 0.404s\n",
      "test time:  0.002s\n",
      "accuracy:   0.629\n",
      "\n",
      "================================================================================\n",
      "kNN\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=10, p=2,\n",
      "           weights='uniform')\n",
      "train time: 0.021s\n",
      "test time:  2.138s\n",
      "accuracy:   0.642\n",
      "\n",
      "================================================================================\n",
      "Random forest\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "train time: 164.386s\n",
      "test time:  0.484s\n",
      "accuracy:   0.625\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for clf, name in (\n",
    "        (RidgeClassifier(tol=1e-2, solver=\"lsqr\"), \"Ridge Classifier\"),\n",
    "        (Perceptron(n_iter=50), \"Perceptron\"),\n",
    "        (PassiveAggressiveClassifier(n_iter=50), \"Passive-Aggressive\"),\n",
    "        (KNeighborsClassifier(n_neighbors=10), \"kNN\"),\n",
    "        (RandomForestClassifier(n_estimators=100), \"Random forest\")):\n",
    "    print('=' * 80)\n",
    "    print(name)\n",
    "    results.append(benchmark(clf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nic\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Users\\Nic\\Anaconda3\\lib\\site-packages\\sklearn\\grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'kernel': ('linear', 'rbf'), 'C': [1, 10]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm, grid_search\n",
    "parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
    "svr = svm.SVC()\n",
    "clf = grid_search.GridSearchCV(svr, parameters)\n",
    "clf.fit(X_train, y_train)\n",
    "                            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:   0.647\n"
     ]
    }
   ],
   "source": [
    "pred = clf.predict(X_test)\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(\"accuracy:   %0.3f\" % score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing initial model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'ngram_range': ('2,2', '2,3')}\n",
    "mnb = MultinomialNB()\n",
    "clf = grid_search.GridSearchCV(mnb, parameters)\n",
    "clf.fit(X_train, y_train)\n",
    "vectorizer = CountVectorizer(ngram_range=(3,3))\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "baseline_clf = Pipeline([('vect', vectorizer),\n",
    "                         ('tfidf', TfidfTransformer()),\n",
    "                         ('clf', clf),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_clf.fit(df_train['message'], df_train['label'])\n",
    "predicted = baseline_clf.predict(df_test['message'])\n",
    "print (\"Initial baseline accuracy: \" + str(np.mean(predicted == df_test['label'])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
