{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#import bson\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('emails.bson','rb') as f:\n",
    "#    data = bson.decode_all(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('emails.csv')\n",
    "#data[10005]['header_info']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 517401 entries, 0 to 517400\n",
      "Data columns (total 2 columns):\n",
      "file       517401 non-null object\n",
      "message    517401 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 7.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary for abbreviated names/UID to e-mail address mapping\n",
    "abrv_names = {}\n",
    "for row in df['file']:\n",
    "    name = row.split('/')[0]\n",
    "    if not name in abrv_names.keys():\n",
    "        abrv_names[name] = \"Temp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique abbreviated name in file: 150\n",
      "allen-p\n",
      "arnold-j\n",
      "arora-h\n",
      "badeer-r\n",
      "bailey-s\n",
      "bass-e\n",
      "baughman-d\n",
      "beck-s\n",
      "benson-r\n",
      "blair-l\n",
      "brawner-s\n",
      "buy-r\n",
      "campbell-l\n",
      "carson-m\n",
      "cash-m\n",
      "causholli-m\n",
      "corman-s\n",
      "crandell-s\n",
      "cuilla-m\n",
      "dasovich-j\n",
      "davis-d\n",
      "dean-c\n",
      "delainey-d\n",
      "derrick-j\n",
      "dickson-s\n",
      "donoho-l\n",
      "donohoe-t\n",
      "dorland-c\n",
      "ermis-f\n",
      "farmer-d\n",
      "fischer-m\n",
      "forney-j\n",
      "fossum-d\n",
      "gang-l\n",
      "gay-r\n",
      "geaccone-t\n",
      "germany-c\n",
      "gilbertsmith-d\n",
      "giron-d\n",
      "griffith-j\n",
      "grigsby-m\n",
      "guzman-m\n",
      "haedicke-m\n",
      "hain-m\n",
      "harris-s\n",
      "hayslett-r\n",
      "heard-m\n",
      "hendrickson-s\n",
      "hernandez-j\n",
      "hodge-j\n",
      "holst-k\n",
      "horton-s\n",
      "hyatt-k\n",
      "hyvl-d\n",
      "jones-t\n",
      "kaminski-v\n",
      "kean-s\n",
      "keavey-p\n",
      "keiser-k\n",
      "king-j\n",
      "kitchen-l\n",
      "kuykendall-t\n",
      "lavorato-j\n",
      "lay-k\n",
      "lenhart-m\n",
      "lewis-a\n",
      "linder-e\n",
      "lokay-m\n",
      "lokey-t\n",
      "love-p\n",
      "lucci-p\n",
      "maggi-m\n",
      "mann-k\n",
      "martin-t\n",
      "may-l\n",
      "mccarty-d\n",
      "mcconnell-m\n",
      "mckay-b\n",
      "mckay-j\n",
      "mclaughlin-e\n",
      "merriss-s\n",
      "meyers-a\n",
      "mims-thurston-p\n",
      "motley-m\n",
      "neal-s\n",
      "nemec-g\n",
      "panus-s\n",
      "parks-j\n",
      "pereira-s\n",
      "perlingiere-d\n",
      "phanis-s\n",
      "pimenov-v\n",
      "platter-p\n",
      "presto-k\n",
      "quenet-j\n",
      "quigley-d\n",
      "rapp-b\n",
      "reitmeyer-j\n",
      "richey-c\n",
      "ring-a\n",
      "ring-r\n",
      "rodrique-r\n",
      "rogers-b\n",
      "ruscitti-k\n",
      "sager-e\n",
      "saibi-e\n",
      "salisbury-h\n",
      "sanchez-m\n",
      "sanders-r\n",
      "scholtes-d\n",
      "schoolcraft-d\n",
      "schwieger-j\n",
      "scott-s\n",
      "semperger-c\n",
      "shackleton-s\n",
      "shankman-j\n",
      "shapiro-r\n",
      "shively-h\n",
      "skilling-j\n",
      "slinger-r\n",
      "smith-m\n",
      "solberg-g\n",
      "south-s\n",
      "staab-t\n",
      "stclair-c\n",
      "steffes-j\n",
      "stepenovitch-j\n",
      "stokley-c\n",
      "storey-g\n",
      "sturm-f\n",
      "swerzbin-m\n",
      "symes-k\n",
      "taylor-m\n",
      "tholt-j\n",
      "thomas-p\n",
      "townsend-j\n",
      "tycholiz-b\n",
      "ward-k\n",
      "watson-k\n",
      "weldon-c\n",
      "whalley-g\n",
      "whalley-l\n",
      "white-s\n",
      "whitt-m\n",
      "williams-j\n",
      "williams-w3\n",
      "wolfe-j\n",
      "ybarbo-p\n",
      "zipper-a\n",
      "zufferli-j\n"
     ]
    }
   ],
   "source": [
    "print (\"Total unique abbreviated name in file: \" + str(len(abrv_names.keys())))\n",
    "for name in abrv_names.keys():\n",
    "    print (name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sent = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build mapping using X-origin as source\n",
    "# Find max sent from name\n",
    "for row in df['message']:\n",
    "    try:\n",
    "        uid = row.split('\\n')[13].split(' ')[1].lower()\n",
    "        from_ = row.split('\\n')[2].split('@')[0].split(' ')[1]\n",
    "\n",
    "        # Only map existing uids\n",
    "        if uid in abrv_names.keys():\n",
    "            # Ignore one-off corner cases for the mapping purposes only\n",
    "            if from_ != 'info' and \\\n",
    "                from_ != 'activetrader' and \\\n",
    "                from_ != 'trader' and \\\n",
    "                from_ != 'office-chairman' and \\\n",
    "                from_ != 'office.chairman' and \\\n",
    "                from_ != 'customer-service' and \\\n",
    "                from_ != 'enron.announcements' and \\\n",
    "                from_ != 'doctor' and \\\n",
    "                from_.find('..') == -1:\n",
    "                    \n",
    "                if not uid in max_sent.keys():\n",
    "                    max_sent[uid] = {}\n",
    "\n",
    "                if not from_ in max_sent[uid].keys():\n",
    "                    max_sent[uid][from_] = 1\n",
    "                else:\n",
    "                    max_sent[uid][from_] += 1\n",
    "\n",
    "    except Exception as e:\n",
    "        None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Associate uid and job title\n",
    "for uid in abrv_names.keys():\n",
    "    max_count = 0\n",
    "    for from_ in max_sent[uid].keys():\n",
    "        if max_sent[uid][from_] > max_count:\n",
    "            abrv_names[uid] = from_\n",
    "            max_count = max_sent[uid][from_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse Job Titles Dataset\n",
    "jt_dict = {}\n",
    "jt_na_list = []\n",
    "with open('enron_data.html', 'r') as file_:\n",
    "    for line in file_.readlines():\n",
    "        if line != '':\n",
    "            # ID UID FName LName Rank\n",
    "            details = line.split()\n",
    "            uid = details[1]\n",
    "            rank = ' '.join(details[4:])\n",
    "            if rank != 'N/A' and rank != 'South N/A':\n",
    "                jt_dict[uid] = rank\n",
    "            else:\n",
    "                jt_na_list.append(uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CEO = 0\n",
    "# C-Suite = 0\n",
    "c_count = 0\n",
    "# President = 1\n",
    "p_count = 0\n",
    "# VP = 2\n",
    "vp_count = 0\n",
    "# Director = 3\n",
    "d_count = 0\n",
    "# IHL = 4\n",
    "ihl_count = 0\n",
    "# Manager = 5\n",
    "m_count = 0\n",
    "# Trader / Specialist / Employee = 6\n",
    "tse_count = 0\n",
    "\n",
    "# UID -> Rank\n",
    "employee_rank = {}\n",
    "\n",
    "for rank in jt_dict.keys():\n",
    "    raw_rank = jt_dict[rank]\n",
    "    if raw_rank.find('CEO') != -1:\n",
    "        #print (jt_dict[rank])\n",
    "        #print (rank)\n",
    "        employee_rank[rank] = 0\n",
    "        c_count += 1\n",
    "    elif raw_rank.find('Chief') != -1:\n",
    "        #print (jt_dict[rank])\n",
    "        #print (rank)\n",
    "        employee_rank[rank] = 0\n",
    "        c_count += 1\n",
    "    elif raw_rank.find('Vice') != -1:\n",
    "        #print (jt_dict[rank])\n",
    "        #print (rank)\n",
    "        employee_rank[rank] = 2\n",
    "        vp_count += 1\n",
    "    elif raw_rank.find('President') != -1:\n",
    "        #print (jt_dict[rank])\n",
    "        #print (rank)\n",
    "        employee_rank[rank] = 1\n",
    "        p_count += 1\n",
    "    elif raw_rank.find('Director') != -1:\n",
    "        #print (jt_dict[rank])\n",
    "        #print (rank)\n",
    "        employee_rank[rank] = 3\n",
    "        d_count += 1\n",
    "    elif raw_rank.find('Lawyer') != -1:\n",
    "        #print (jt_dict[rank])\n",
    "        #print (rank)\n",
    "        employee_rank[rank] = 4\n",
    "        ihl_count += 1\n",
    "    elif raw_rank.find('Manager') != -1:\n",
    "        #print (jt_dict[rank])\n",
    "        #print (rank)\n",
    "        employee_rank[rank] = 5\n",
    "        m_count += 1\n",
    "    else:\n",
    "        #print (jt_dict[rank])\n",
    "        #print (rank)\n",
    "        employee_rank[rank] = 6\n",
    "        tse_count += 1        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C Level: 8\n",
      "Presidents: 4\n",
      "VPs: 17\n",
      "Directors: 16\n",
      "In-House Lawyers: 3\n",
      "Managers: 9\n",
      "T/S/Es: 51\n"
     ]
    }
   ],
   "source": [
    "print ('C Level: ' + str(c_count))\n",
    "print ('Presidents: ' + str(p_count))\n",
    "print ('VPs: ' + str(vp_count))\n",
    "print ('Directors: ' + str(d_count))\n",
    "print ('In-House Lawyers: ' + str(ihl_count))\n",
    "print ('Managers: ' + str(m_count))\n",
    "print ('T/S/Es: ' + str(tse_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'phillip.allen': 'allen-p',\n",
       " 'john.arnold': 'arnold-j',\n",
       " 'harry.arora': 'arora-h',\n",
       " 'robert.badeer': 'badeer-r',\n",
       " 'susan.bailey': 'bailey-s',\n",
       " 'eric.bass': 'bass-e',\n",
       " 'don.baughman': 'baughman-d',\n",
       " 'sally.beck': 'beck-s',\n",
       " 'robert.benson': 'benson-r',\n",
       " 'lynn.blair': 'blair-l',\n",
       " 'sandra.brawner': 'brawner-s',\n",
       " 'rick.buy': 'buy-r',\n",
       " 'larry.campbell': 'campbell-l',\n",
       " 'mike.carson': 'carson-m',\n",
       " 'michelle.cash': 'cash-m',\n",
       " 'monika.causholli': 'causholli-m',\n",
       " 'shelley.corman': 'corman-s',\n",
       " 'sean.crandall': 'crandell-s',\n",
       " 'martin.cuilla': 'cuilla-m',\n",
       " 'jeff.dasovich': 'dasovich-j',\n",
       " 'dana.davis': 'davis-d',\n",
       " 'dgagliardi': 'dean-c',\n",
       " 'david.delainey': 'delainey-d',\n",
       " 'james.derrick': 'derrick-j',\n",
       " 'stacy.dickson': 'dickson-s',\n",
       " 'lindy.donoho': 'donoho-l',\n",
       " 'tom.donohoe': 'donohoe-t',\n",
       " 'chris.dorland': 'dorland-c',\n",
       " 'noreply': 'lucci-p',\n",
       " 'daren.farmer': 'farmer-d',\n",
       " 'jeff.duff': 'fischer-m',\n",
       " 'john.forney': 'forney-j',\n",
       " 'drew.fossum': 'fossum-d',\n",
       " 'oasis502': 'gang-l',\n",
       " 'randall.gay': 'gay-r',\n",
       " 'tracy.geaccone': 'geaccone-t',\n",
       " 'chris.germany': 'germany-c',\n",
       " 'doug.gilbert-smith': 'gilbertsmith-d',\n",
       " 'darron.giron': 'giron-d',\n",
       " 'soblander': 'keavey-p',\n",
       " 'mike.grigsby': 'grigsby-m',\n",
       " 'mark.guzman': 'guzman-m',\n",
       " 'mark.haedicke': 'haedicke-m',\n",
       " 'mary.hain': 'hain-m',\n",
       " 'steven.harris': 'harris-s',\n",
       " 'rod.hayslett': 'hayslett-r',\n",
       " 'marie.heard': 'heard-m',\n",
       " 'scott.hendrickson': 'hendrickson-s',\n",
       " 'judy.hernandez': 'hernandez-j',\n",
       " 'navigator': 'pimenov-v',\n",
       " 'feedback': 'holst-k',\n",
       " 'stanley.horton': 'horton-s',\n",
       " 'kevin.hyatt': 'hyatt-k',\n",
       " 'dan.hyvl': 'hyvl-d',\n",
       " 'tana.jones': 'jones-t',\n",
       " 'vince.kaminski': 'kaminski-v',\n",
       " 'steven.kean': 'kean-s',\n",
       " 'kam.keiser': 'keiser-k',\n",
       " 'neilanderson': 'king-j',\n",
       " 'louise.kitchen': 'kitchen-l',\n",
       " 'tori.kuykendall': 'kuykendall-t',\n",
       " 'john.lavorato': 'lavorato-j',\n",
       " 'rosalee.fleming': 'lay-k',\n",
       " 'matthew.lenhart': 'lenhart-m',\n",
       " 'alerts': 'lewis-a',\n",
       " 'bill.iii': 'merriss-s',\n",
       " 'michelle.lokay': 'lokay-m',\n",
       " 'teb.lokey': 'lokey-t',\n",
       " 'phillip.love': 'love-p',\n",
       " 'mike.maggi': 'maggi-m',\n",
       " 'kay.mann': 'mann-k',\n",
       " 'thomas.martin': 'martin-t',\n",
       " 'larry.may': 'may-l',\n",
       " 'danny.mccarty': 'mccarty-d',\n",
       " 'mike.mcconnell': 'mcconnell-m',\n",
       " 'brad.mckay': 'mckay-b',\n",
       " 'jonathan.mckay': 'mckay-j',\n",
       " 'errol.mclaughlin': 'mclaughlin-e',\n",
       " 'bert.meyers': 'meyers-a',\n",
       " 'patrice.mims': 'mims-thurston-p',\n",
       " 'nytdirect': 'thomas-p',\n",
       " 'scott.neal': 'neal-s',\n",
       " 'gerald.nemec': 'nemec-g',\n",
       " 'stephanie.panus': 'phanis-s',\n",
       " 'joe.parks': 'parks-j',\n",
       " 'susan.pereira': 'pereira-s',\n",
       " 'debra.perlingiere': 'perlingiere-d',\n",
       " 'phillip.platter': 'platter-p',\n",
       " 'kevin.presto': 'presto-k',\n",
       " 'joe.quenet': 'quenet-j',\n",
       " 'dutch.quigley': 'quigley-d',\n",
       " 'bill.rapp': 'rapp-b',\n",
       " 'jay.reitmeyer': 'reitmeyer-j',\n",
       " 'cooper.richey': 'richey-c',\n",
       " 'andrea.ring': 'ring-a',\n",
       " 'richard.ring': 'ring-r',\n",
       " 'robin.rodrigue': 'rodrique-r',\n",
       " 'benjamin.rogers': 'rogers-b',\n",
       " 'kevin.ruscitti': 'ruscitti-k',\n",
       " 'elizabeth.sager': 'sager-e',\n",
       " 'gelliott': 'saibi-e',\n",
       " 'holden.salisbury': 'salisbury-h',\n",
       " 'monique.sanchez': 'sanchez-m',\n",
       " 'richard.sanders': 'sanders-r',\n",
       " 'diana.scholtes': 'scholtes-d',\n",
       " 'darrell.schoolcraft': 'schoolcraft-d',\n",
       " 'jim.schwieger': 'schwieger-j',\n",
       " 'susan.scott': 'scott-s',\n",
       " 'cara.semperger': 'semperger-c',\n",
       " 'sara.shackleton': 'shackleton-s',\n",
       " 'jeffrey.shankman': 'shankman-j',\n",
       " 'richard.shapiro': 'shapiro-r',\n",
       " 'hunter.shively': 'shively-h',\n",
       " 'sherri.sera': 'skilling-j',\n",
       " 'ryan.slinger': 'slinger-r',\n",
       " 'matt.smith': 'smith-m',\n",
       " 'geir.solberg': 'solberg-g',\n",
       " 'steven.south': 'south-s',\n",
       " 'theresa.staab': 'staab-t',\n",
       " 'carol.clair': 'stclair-c',\n",
       " 'joann.scott': 'steffes-j',\n",
       " 'joe.stepenovitch': 'stepenovitch-j',\n",
       " 'chris.stokley': 'stokley-c',\n",
       " 'geoff.storey': 'storey-g',\n",
       " 'fletcher.sturm': 'sturm-f',\n",
       " 'mike.swerzbin': 'swerzbin-m',\n",
       " 'kate.symes': 'symes-k',\n",
       " 'mark.taylor': 'taylor-m',\n",
       " 'jane.tholt': 'tholt-j',\n",
       " 'judy.townsend': 'townsend-j',\n",
       " 'barry.tycholiz': 'tycholiz-b',\n",
       " 'houston': 'ward-k',\n",
       " 'kimberly.watson': 'watson-k',\n",
       " 'charles.weldon': 'weldon-c',\n",
       " 'greg.whalley': 'whalley-g',\n",
       " 'liz.taylor': 'whalley-l',\n",
       " 'john.postlethwaite': 'white-s',\n",
       " 'mark.whitt': 'whitt-m',\n",
       " 'jason.williams': 'williams-j',\n",
       " 'bill.williams': 'williams-w3',\n",
       " 'beth.cherry': 'wolfe-j',\n",
       " \"paul.y'barbo\": 'ybarbo-p',\n",
       " 'andy.zipper': 'zipper-a',\n",
       " 'john.zufferli': 'zufferli-j'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse_abrv_names = {v : k for k, v in abrv_names.items()}\n",
    "reverse_abrv_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from_:to_ {message}\n",
    "#unique to_ for testing\n",
    "#to_set = set([])\n",
    "\n",
    "from_to_ = {}\n",
    "for index, row in df.iterrows():\n",
    "    row = row['message']\n",
    "    from_ = row.split('\\n')[2].split('@')[0].split(' ')[1]\n",
    "    to_ = row.split('\\n')[3].split('@')[0].split(' ')[1]\n",
    "    \n",
    "    #to_set.add(to_)\n",
    "    if to_ in reverse_abrv_names.keys() and from_ in reverse_abrv_names.keys() and to_ != from_:\n",
    "        f_uid = reverse_abrv_names[from_]\n",
    "        t_uid = reverse_abrv_names[to_]\n",
    "        if f_uid in employee_rank.keys() and t_uid in employee_rank.keys():\n",
    "            try:\n",
    "                subject = row.split('\\n')[4].split(' ')[1]\n",
    "            except:\n",
    "                subject = row.split('\\n')[4]\n",
    "\n",
    "            message = ''.join(row.split('\\n')[15:])\n",
    "            if message.find('Forwarded') != -1:\n",
    "                message = 'Forwarded'\n",
    "\n",
    "            rlshp = from_ + ':' + to_\n",
    "            if rlshp in from_to_.keys():\n",
    "                from_to_[rlshp] += ' ' + subject + ' ' + message\n",
    "            else:\n",
    "                from_to_[rlshp] = subject + ' ' + message        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Relationships: 719\n"
     ]
    }
   ],
   "source": [
    "print (\"# Relationships: \" + str(len(from_to_.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to label -> text\n",
    "pre_prep_format = []\n",
    "u_count = 0\n",
    "nu_count = 0\n",
    "\n",
    "for key in from_to_.keys():\n",
    "    from_, to_ = key.split(':')\n",
    "    f_uid = reverse_abrv_names[from_]\n",
    "    f_rank = employee_rank[f_uid]\n",
    "    t_uid = reverse_abrv_names[to_]\n",
    "    t_rank = employee_rank[t_uid]\n",
    "    \n",
    "    if f_rank > t_rank:\n",
    "        pre_prep_format.append(('upward', from_to_[key]))\n",
    "        u_count += 1\n",
    "    elif f_rank < t_rank:\n",
    "        nu_count += 1\n",
    "        pre_prep_format.append(('not-upward', from_to_[key]))\n",
    "    #else:\n",
    "    #    pre_prep_format.append(('neutral', from_to_[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labelled messages: 494\n",
      "Upward messages: 256\n",
      "Not-upward messages: 238\n"
     ]
    }
   ],
   "source": [
    "print (\"Labelled messages: \" + str(len(pre_prep_format)))\n",
    "print (\"Upward messages: \" + str(u_count))\n",
    "print (\"Not-upward messages: \" + str(nu_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Nic\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Nic\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Nic\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "post_prep_format = []\n",
    "\n",
    "for label, message in pre_prep_format:\n",
    "    processed_message = message.lower()\n",
    "    \n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(processed_message)\n",
    "    \n",
    "    filtered_words = filter(lambda token: token not in stopwords.words('english'), tokens)\n",
    "    \n",
    "    # Return\n",
    "    post_prep_format.append((label, ' '.join(filtered_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print (pre_prep_format[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print (post_prep_format[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(post_prep_format, columns=['label', 'message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df[:round(df.shape[0] *.7)]\n",
    "df_test = df[round(df.shape[0] *.7):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "baseline_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                         ('tfidf', TfidfTransformer()),\n",
    "                         ('clf', MultinomialNB()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_clf.fit(df_train['message'], df_train['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial baseline accuracy: 0.5202702702702703\n"
     ]
    }
   ],
   "source": [
    "predicted = baseline_clf.predict(df_test['message'])\n",
    "print (\"Initial baseline accuracy: \" + str(np.mean(predicted == df_test['label'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(346,)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['message'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = set([])\n",
    "\n",
    "for message in df_train['message']:\n",
    "    word_count.add(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "341"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import sklearn.datasets as skds\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = 2\n",
    "vocab_size = 25000\n",
    "batch_size = 100\n",
    " \n",
    "# define Tokenizer with Vocab Size\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(df_train['message'])\n",
    " \n",
    "x_train = tokenizer.texts_to_matrix(df_train['message'], mode='tfidf')\n",
    "x_test = tokenizer.texts_to_matrix(df_test['message'], mode='tfidf')\n",
    " \n",
    "encoder = LabelBinarizer()\n",
    "encoder.fit(df_train['label'])\n",
    "y_train = encoder.transform(df_train['label'])\n",
    "y_test = encoder.transform(df_test['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_19 (Dense)             (None, 512)               12800512  \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 2)                 1026      \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 13,064,194\n",
      "Trainable params: 13,064,194\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 311 samples, validate on 35 samples\n",
      "Epoch 1/30\n",
      "311/311 [==============================] - 3s 9ms/step - loss: 1.2787 - acc: 0.5177 - val_loss: 0.8892 - val_acc: 0.5143\n",
      "Epoch 2/30\n",
      "311/311 [==============================] - 1s 4ms/step - loss: 0.6515 - acc: 0.7942 - val_loss: 1.4597 - val_acc: 0.4571\n",
      "Epoch 3/30\n",
      "311/311 [==============================] - 1s 3ms/step - loss: 0.6937 - acc: 0.8617 - val_loss: 1.1051 - val_acc: 0.5429\n",
      "Epoch 4/30\n",
      "311/311 [==============================] - 1s 3ms/step - loss: 0.2933 - acc: 0.9003 - val_loss: 1.3175 - val_acc: 0.4857\n",
      "Epoch 5/30\n",
      "311/311 [==============================] - 1s 3ms/step - loss: 0.3867 - acc: 0.9068 - val_loss: 1.1899 - val_acc: 0.5429\n",
      "Epoch 6/30\n",
      "311/311 [==============================] - 1s 4ms/step - loss: 0.3416 - acc: 0.9357 - val_loss: 1.8210 - val_acc: 0.4000\n",
      "Epoch 7/30\n",
      "311/311 [==============================] - 1s 4ms/step - loss: 0.3662 - acc: 0.9518 - val_loss: 2.2133 - val_acc: 0.4000\n",
      "Epoch 8/30\n",
      "311/311 [==============================] - 1s 4ms/step - loss: 0.3966 - acc: 0.9421 - val_loss: 1.7081 - val_acc: 0.4571\n",
      "Epoch 9/30\n",
      "311/311 [==============================] - 1s 4ms/step - loss: 0.3436 - acc: 0.9486 - val_loss: 1.4186 - val_acc: 0.5143\n",
      "Epoch 10/30\n",
      "311/311 [==============================] - 1s 4ms/step - loss: 0.1926 - acc: 0.9678 - val_loss: 1.7035 - val_acc: 0.4857\n",
      "Epoch 11/30\n",
      "311/311 [==============================] - 1s 4ms/step - loss: 0.2539 - acc: 0.9614 - val_loss: 1.9518 - val_acc: 0.4857\n",
      "Epoch 12/30\n",
      "311/311 [==============================] - 1s 4ms/step - loss: 0.2026 - acc: 0.9614 - val_loss: 2.1457 - val_acc: 0.5143\n",
      "Epoch 13/30\n",
      "311/311 [==============================] - 1s 4ms/step - loss: 0.2865 - acc: 0.9646 - val_loss: 2.3544 - val_acc: 0.5143\n",
      "Epoch 14/30\n",
      "311/311 [==============================] - 1s 4ms/step - loss: 0.2796 - acc: 0.9678 - val_loss: 2.3841 - val_acc: 0.5143\n",
      "Epoch 15/30\n",
      "311/311 [==============================] - 1s 4ms/step - loss: 0.2224 - acc: 0.9711 - val_loss: 2.3893 - val_acc: 0.4571\n",
      "Epoch 16/30\n",
      "311/311 [==============================] - 1s 4ms/step - loss: 0.2858 - acc: 0.9614 - val_loss: 2.2220 - val_acc: 0.4571\n",
      "Epoch 17/30\n",
      "311/311 [==============================] - 1s 4ms/step - loss: 0.3019 - acc: 0.9678 - val_loss: 2.2229 - val_acc: 0.4286\n",
      "Epoch 18/30\n",
      "311/311 [==============================] - 1s 4ms/step - loss: 0.2893 - acc: 0.9614 - val_loss: 2.2004 - val_acc: 0.4286\n",
      "Epoch 19/30\n",
      "311/311 [==============================] - 1s 4ms/step - loss: 0.1811 - acc: 0.9711 - val_loss: 2.3527 - val_acc: 0.5143\n",
      "Epoch 20/30\n",
      "311/311 [==============================] - 1s 4ms/step - loss: 0.2439 - acc: 0.9582 - val_loss: 2.4427 - val_acc: 0.5143\n",
      "Epoch 21/30\n",
      "311/311 [==============================] - 1s 4ms/step - loss: 0.2553 - acc: 0.9678 - val_loss: 2.4192 - val_acc: 0.4571\n",
      "Epoch 22/30\n",
      "311/311 [==============================] - 1s 4ms/step - loss: 0.2246 - acc: 0.9711 - val_loss: 2.4083 - val_acc: 0.4857\n",
      "Epoch 23/30\n",
      "311/311 [==============================] - 1s 4ms/step - loss: 0.2665 - acc: 0.9711 - val_loss: 2.4123 - val_acc: 0.4571\n",
      "Epoch 24/30\n",
      "311/311 [==============================] - 1s 4ms/step - loss: 0.3505 - acc: 0.9678 - val_loss: 2.4171 - val_acc: 0.4571\n",
      "Epoch 25/30\n",
      "311/311 [==============================] - 1s 4ms/step - loss: 0.3075 - acc: 0.9646 - val_loss: 2.4628 - val_acc: 0.4286\n",
      "Epoch 26/30\n",
      "311/311 [==============================] - 1s 4ms/step - loss: 0.2801 - acc: 0.9678 - val_loss: 3.3978 - val_acc: 0.5143\n",
      "Epoch 27/30\n",
      "311/311 [==============================] - 1s 4ms/step - loss: 0.4980 - acc: 0.9518 - val_loss: 3.6183 - val_acc: 0.4857\n",
      "Epoch 28/30\n",
      "311/311 [==============================] - 1s 4ms/step - loss: 0.2583 - acc: 0.9743 - val_loss: 3.4184 - val_acc: 0.5714\n",
      "Epoch 29/30\n",
      "311/311 [==============================] - 1s 4ms/step - loss: 0.2690 - acc: 0.9614 - val_loss: 4.9685 - val_acc: 0.4571\n",
      "Epoch 30/30\n",
      "311/311 [==============================] - 1s 4ms/step - loss: 0.3624 - acc: 0.9518 - val_loss: 4.5378 - val_acc: 0.4857\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(vocab_size,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(num_labels))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=30,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
